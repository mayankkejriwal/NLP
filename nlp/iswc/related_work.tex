Wikification has received increasing research attention since the paper by Bunescu and Pasca in 2006 \cite{bunescu}. They used an SVM kernel \cite{svm}, and the context around the ambiguous mention for disambiguation. However, a different model had to be trained for each mention. This limited the solution to only those ambiguous mentions on which an SVM model had already been trained. The algorithm couldn't be applied to mentions that had never been seen before (and trained on) in the training data. Mihalcae and Csomai published an influential work in 2007 that used Word Sense Disambiguation (WSD) \cite{wsd} to perform entity disambiguation and linking \cite{wikify}. Two techniques used by their proposed system, \emph{Wikify!}, included computing lexical overlap between candidate Wikipedia pages and the context of the ambiguous mention, and training a Naive Bayes classifier using ground truth from Wikipedia \emph{itself}. It is commonly observed that a Wikipedia page contains hyperlinks to other Wikipedia pages. Thus, Wikipedia pages themselves provide valuable training data, which was exploited by the supervised system in that paper.

Since 2007, increasingly sophisticated methods have been proposed to solve the problem, at the price of increasing complexity\cite{milne},\cite{cucerzan},\cite{roth},\cite{global1}. Specifically, such methods espoused moving from \emph{local} methods to \emph{global} methods. Examples of local methods are the works by Bunescu and Pasca \cite{bunescu} and the \emph{Wikify!} system \cite{wikify}. Such methods disambiguate each entity mention \emph{in isolation}. The disambiguation of the $p^{th}$ entity mention is therefore independent of the disambiguation of the $(p+1)^{th}$ entity mention. On the other hand, global methods propose to increase accuracy by performing \emph{collective} disambiguation \cite{global1}. That is, the disambiguation of an entity is now dependent on the disambiguation of other entities in the text. This leads to significant improvements if the document has \emph{thematic coherence}, which is a characteristic of many real-world documents.

Ratinov et al. formulate the problem precisely in their work on D2W \cite{roth}, and show that existing systems adhere to their formulation. Furthermore, they show that an optimal solution to the global problem is NP-hard. The differences in existing systems proposed were those involving the features they employed and also, the approximation technique they used. One important point to note is that all systems assume supervision. That is, the approximation algorithm (typically framed as a machine learning classification problem) assumes a training set, which in the spirit of the \emph{Wikify!} system, is often derived from Wikipedia itself. Although our methods and features are considerably simpler, their novelty lies in their unsupervised application and instantaneous run-time. The wikification is achieved on-the-fly. Since it is online, it can use the most recent version of Wikipedia, as opposed to previous work that relied on training data from potentially outdated dumps.   

In the Semantic Web community, an important, related research direction is automatic extraction and transformation of entities in unstructured documents, and subsequent linking to DBpedia\footnote{dbpedia.org}. A good example of such a system is LODifier \cite{lodifier}. Another effort is the work by Exner and Nugues \cite{extraction}. Although we do not use DBpedia in this paper, the principles are similar and tools like DBpedia Spotlight\footnote{\url{https://github.com/dbpedia-spotlight/dbpedia-spotlight/wiki}} can be used for online annotation in a similar manner as the Wikipedia API. In the rest of this work, we restrict attention to the Wikipedia knowledge base and API. 
